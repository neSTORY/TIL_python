# 정규화와 표준화

아래 내용은 dv_day3를 복습한 내용이다.

---

**정규화와 표준화를 왜 할까?**

데이터들의 각 특성과 범위는 다를 수 밖에 없다.

예를 들어 키와 몸무게의 데이터가 있다하면 키의 단위는 cm, 몸무게의 단위는 kg이다. 

이 데이터들을 변환없이 그냥 비교하려면 단위가 달라서 비교하기 무리가 있다.

혹은 같은 단위를 사용한다 하더라도

데이터들의 범위가 달라서 비교할 수 없을 경우도 있다.

예를 들면 한 데이터는 100점 만점을 기준으로 한 시험점수이고 다른 데이터는 1000점 만점을 기준으로 한 시험점수라면

두개의 데이터를 비교하기 무리가 있다.

이렇게 단위 혹은 범위가 다른 데이터를 비교하기 위해서 같은 범위로 만들어 줄 필요가 있다.

그래서 우리는 정규화 또는 표준화를 사용한다.

----



## 정규화(normalization)

정규화는 범위를 0 ~ 1로 만들어주는 역할을 한다.

기본적인 공식은 (x - x.min) / (x.max - x.min)이다.



## 표준화(standardization)

데이터들의 평균을 0, 표준편차를 1로 갖게 바꿔주는 역할을 한다.

기본적인 공식은 (x - x.mean) / x.std 이다.



---



## How to Standardiztion in Python

그렇다면 파이썬에서 표준화를 어떻게 할 수 있는지 알아보자

1. numpy 사용

   우리는 넘파이를 이용하여 데이터의 표준화를 이뤄낼 수 있다. 

   넘파이의 함수 mean과 std를 사용하면 가능하다.

   

   넘파이를 import하고 random.randint 함수를 사용하여 6행*5열의 데이터를 만들어보자

   ```python
   import numpy as np
   
   data = np.random.randint(30, size=(6,5))
   data
   ```

   > ```
   > array([[18, 13,  9, 14,  6],
   >        [20, 22, 29, 18, 20],
   >        [14,  7,  8,  0, 27],
   >        [23,  3, 28,  4, 28],
   >        [ 4, 15, 11, 19, 10],
   >        [14, 27,  4, 21, 15]])
   > ```



넘파이의 특징중 하나인 브로드캐스팅을 살펴보자.

```python
data-1
```

> ```
> array([[17, 12,  8, 13,  5],
>        [19, 21, 28, 17, 19],
>        [13,  6,  7, -1, 26],
>        [22,  2, 27,  3, 27],
>        [ 3, 14, 10, 18,  9],
>        [13, 26,  3, 20, 14]])
> ```



우리는 data라는 배열에 -1만 해줬을 뿐인데 모든 값들이 -1을 한 결과를 얻을 수 있다.

이런 특성이 브로드캐스팅이다.



이 특성을 이용하여 넘파이를 통한 표준화를 할 수 있다.

**여기서 axis=0 은 열을 기준, axis = 1은 행을 기준으로 계산해준다.**

```python
data_s1 = (data - np.mean(data, axis = 0 )) / np.std(data, axis = 0)
data_s1
```

> ```
> array([[-1.09539553, -1.42997282, -0.13123574, -0.56360186, -1.03464225],
>        [-0.25636917,  1.02140915,  1.8373004 , -1.87867287,  1.839364  ],
>        [-0.9555578 , -0.88522127, -0.72179659,  1.12720372, -0.11496025],
>        [ 1.42168356, -0.47665761, -0.9186502 ,  0.56360186,  0.459841  ],
>        [ 1.28184583,  0.47665761,  0.75460552,  0.        , -1.1496025 ],
>        [-0.39620689,  1.29378493, -0.82022339,  0.75146915,  0.        ]])
> ```



2.  scipy 라이브러리를 이용한 표준화

   scipy의 stats에서 zscore라는 함수를 이용하면 표준화를 할 수 있다.

   ```python
   import scipy.stats as ss
   
   data_s2 = ss.zscore(data)
   data_s2
   ```

   > ```
   > array([[-1.09539553, -1.42997282, -0.13123574, -0.56360186, -1.03464225],
   >        [-0.25636917,  1.02140915,  1.8373004 , -1.87867287,  1.839364  ],
   >        [-0.9555578 , -0.88522127, -0.72179659,  1.12720372, -0.11496025],
   >        [ 1.42168356, -0.47665761, -0.9186502 ,  0.56360186,  0.459841  ],
   >        [ 1.28184583,  0.47665761,  0.75460552,  0.        , -1.1496025 ],
   >        [-0.39620689,  1.29378493, -0.82022339,  0.75146915,  0.        ]])
   > ```

위의 넘파이를 이용하는 것 보다 훨씬 편하게 계산할 수 있다.



3. sklearn 라이브러리를 이용한 표준화

   scipy 처럼 라이브러리 내 함수를 이용하여 표준화를 진행하는 방법이다.

   ```python
   from sklearn.preprocessing import StandardScaler
   
   data_s3 = StandardScaler().fit_transform(data)
   data_s3
   ```

   > ```
   > array([[-1.09539553, -1.42997282, -0.13123574, -0.56360186, -1.03464225],
   >        [-0.25636917,  1.02140915,  1.8373004 , -1.87867287,  1.839364  ],
   >        [-0.9555578 , -0.88522127, -0.72179659,  1.12720372, -0.11496025],
   >        [ 1.42168356, -0.47665761, -0.9186502 ,  0.56360186,  0.459841  ],
   >        [ 1.28184583,  0.47665761,  0.75460552,  0.        , -1.1496025 ],
   >        [-0.39620689,  1.29378493, -0.82022339,  0.75146915,  0.        ]])
   > ```



---

